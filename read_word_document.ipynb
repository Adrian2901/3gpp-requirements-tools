{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXP48tIsZTyR"
      },
      "source": [
        "# Reading MS Word document\n",
        "\n",
        "We read the document and create a .csv file with the list of paragaphs and their lengths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
        "os.environ['TOKENIZERS_PARALLELISM'] = 'true'\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
        "os.environ['TORCH_USE_CUDA_DSA'] = 'true'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bu8tMVzbab1"
      },
      "outputs": [],
      "source": [
        "docTitle = \"22289.docx\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xm9i5GSybNIt"
      },
      "source": [
        "## Solution 2: docx\n",
        "\n",
        "This is a library that understands MS Word styles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "toZhV25ubSVb"
      },
      "outputs": [],
      "source": [
        "# !pip install -q docx2python\n",
        "from docx2python import docx2python\n",
        "from transformers import pipeline, BartForConditionalGeneration, BartTokenizer, PegasusTokenizer, PegasusForConditionalGeneration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYihJbqseeBC"
      },
      "outputs": [],
      "source": [
        "# extract docx content\n",
        "doc_result = docx2python(docTitle, paragraph_styles = True, html=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJScbyZ-cRiM",
        "outputId": "378f04d7-71b0-41bf-bc8e-0a2cfbb4c01a"
      },
      "outputs": [],
      "source": [
        "print(doc_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9o1Fww3Df21I",
        "outputId": "f1974c4d-f49f-4819-82b0-210aa1d8d27e"
      },
      "outputs": [],
      "source": [
        "# this part of the code identifies the section that contains the word \"latency\"\n",
        "# and then extracts the lines that contain the word \"latency\" and all other lines in the same section\n",
        "strSectionTitle = \"\"\n",
        "dictSections = {}\n",
        "listLatency = []\n",
        "listLatencyLines = []\n",
        "for oneLine in doc_result.text.split('\\n'):\n",
        "  if \"<h\" in oneLine:\n",
        "    strSectionTitle = oneLine\n",
        "    dictSections[strSectionTitle] = []\n",
        "\n",
        "  if strSectionTitle != \"\":  \n",
        "    dictSections[strSectionTitle].append(oneLine)\n",
        "\n",
        "  if \"latency\" in oneLine: \n",
        "    listLatency.append(strSectionTitle)\n",
        "    listLatencyLines.append(oneLine)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# go through all the lines in the section that contains the word \"latency\"\n",
        "for title in listLatency:\n",
        "  if title in dictSections.keys():\n",
        "    lstLines = dictSections[title]\n",
        "    # join lines in the list to one long string\n",
        "    strLines = ' '.join(lstLines)\n",
        "    # print(strLines) but only 20 first characters\n",
        "    print(strLines)\n",
        "\n",
        "    summary = getSummaryBART(strLines)\n",
        "    \n",
        "    # summary = getSummaryDistilBart(strLines, 30)\n",
        "    \n",
        "    print(summary)\n",
        "    \n",
        "    print (\"-----------------\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_name_bart = \"knkarthick/MEETING-SUMMARY-BART-LARGE-XSUM-SAMSUM-DIALOGSUM-AMI\"\n",
        "tokenizer_bart = BartTokenizer.from_pretrained(model_name_bart)\n",
        "model_bart = BartForConditionalGeneration.from_pretrained(model_name_bart).to('cuda')\n",
        "\n",
        "def getSummaryBART(line):\n",
        "\n",
        "    inputs = tokenizer_bart.encode(\"summarize: \" + line, return_tensors=\"pt\", max_length=1024, truncation=True).to('cuda')\n",
        "    summary_ids = model_bart.generate(inputs, max_length=30, min_length=5, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "    summary = tokenizer_bart.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    return summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_name_pegassus = \"google/pegasus-xsum\"\n",
        "tokenizer_pegassus = PegasusTokenizer.from_pretrained(model_name_pegassus)\n",
        "model_pegassus = PegasusForConditionalGeneration.from_pretrained(model_name_pegassus).to('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def getSummaryPegasus(line):\n",
        "    inputs = tokenizer_pegassus.encode(\"summarize: \" + line, return_tensors=\"pt\", max_length=1024, truncation=True).to('cuda')\n",
        "    summary_ids = model_pegassus.generate(inputs, max_length=20, min_length=1, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "    summary = tokenizer_pegassus.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    return summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_name_fb = \"nickmuchi/fb-bart-large-finetuned-trade-the-event-finance-summarizer\"\n",
        "tokenizer_fb = BartTokenizer.from_pretrained(model_name_fb)\n",
        "model_fb = BartForConditionalGeneration.from_pretrained(model_name_fb).to('cuda:1')\n",
        "\n",
        "def getSummaryBart2(line):\n",
        "    inputs = tokenizer_fb.encode(line, return_tensors=\"pt\", max_length=1024, truncation=True).to('cuda:1')\n",
        "    summary_ids = model_fb.generate(inputs, max_length=150, min_length=5, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "    summary = tokenizer_fb.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    return summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_name_db = \"sshleifer/distilbart-cnn-12-6\"\n",
        "tokenizer_db = BartTokenizer.from_pretrained(model_name_db)\n",
        "model_db = BartForConditionalGeneration.from_pretrained(model_name_db).to('cuda')\n",
        "\n",
        "def getSummaryDistilBart(line, length):\n",
        "    inputs = tokenizer_db.encode(line, return_tensors=\"pt\", max_length=1024, truncation=True).to('cuda')\n",
        "    summary_ids = model_db.generate(inputs, max_length=length, min_length=5, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "    summary = tokenizer_db.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    return summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "dictResults = {}\n",
        "iCounter = 0\n",
        "\n",
        "for eachLine in listLatencyLines:\n",
        "    iCounter += 1\n",
        "    summary = getSummaryBART(eachLine)\n",
        "    summary2 = getSummaryDistilBart(eachLine)\n",
        "    summary3 = getSummaryBart2(eachLine)\n",
        "    summary4 = getSummaryPegasus(eachLine)\n",
        "    # print(f'-- line: {eachLine} \\nsummary Bart: {summary}\\nsummary distBart: {summary2}\\nsummary bart2: {summary3} \\n')\n",
        "\n",
        "    # print the number of items processed every 10 items\n",
        "    if iCounter % 10 == 0:\n",
        "        print(f'-- processed {iCounter} items')\n",
        "        \n",
        "    dictResults[eachLine] = [summary, summary2, summary3, summary4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# number of keys in the dictionary is \n",
        "print(f'-- number of keys in the dictionary is {len(dictResults.keys())}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import Levenshtein as lev\n",
        "import pandas as pd\n",
        "\n",
        "# Function to calculate Jaccard similarity\n",
        "def jaccard_similarity(list1, list2):\n",
        "    s1 = set(list1)\n",
        "    s2 = set(list2)\n",
        "    return len(s1.intersection(s2)) / len(s1.union(s2))\n",
        "\n",
        "dictResult = {} \n",
        "\n",
        "for key in dictResults.keys():\n",
        "    # Levenshtein distances\n",
        "    lev1 = lev.distance(dictResults[key][0], dictResults[key][1])\n",
        "    lev2 = lev.distance(dictResults[key][0], dictResults[key][2])\n",
        "    lev3 = lev.distance(dictResults[key][0], dictResults[key][3])\n",
        "    lev4 = lev.distance(dictResults[key][1], dictResults[key][2])\n",
        "    lev5 = lev.distance(dictResults[key][1], dictResults[key][3])\n",
        "    lev6 = lev.distance(dictResults[key][2], dictResults[key][3])\n",
        "\n",
        "    # Jaccard similarities\n",
        "    jac1 = jaccard_similarity(dictResults[key][0], dictResults[key][1])\n",
        "    jac2 = jaccard_similarity(dictResults[key][0], dictResults[key][2])\n",
        "    jac3 = jaccard_similarity(dictResults[key][0], dictResults[key][3])\n",
        "    jac4 = jaccard_similarity(dictResults[key][1], dictResults[key][2])\n",
        "    jac5 = jaccard_similarity(dictResults[key][1], dictResults[key][3])\n",
        "    jac6 = jaccard_similarity(dictResults[key][2], dictResults[key][3])\n",
        "\n",
        "    print(f'-- key: {key} \\nlev1: {lev1} \\nlev2: {lev2} \\nlev3: {lev3} \\nlev4: {lev4} \\nlev5: {lev5} \\nlev6: {lev6} \\njac1: {jac1} \\njac2: {jac2} \\njac3: {jac3} \\njac4: {jac4} \\njac5: {jac5} \\njac6: {jac6} \\n')\n",
        "\n",
        "    # make it into a dictionary\n",
        "    dictResult[key.replace('$', '_')] = [lev1, lev2, lev3, lev4, lev5, lev6, jac1, jac2, jac3, jac4, jac5, jac6]\n",
        "\n",
        "# make the dictionary into a dataframe\n",
        "df = pd.DataFrame.from_dict(dictResult, orient='index', columns=['lev1', 'lev2', 'lev3', 'lev4', 'lev5', 'lev6', 'jac1', 'jac2', 'jac3', 'jac4', 'jac5', 'jac6'])\n",
        "\n",
        "# name the index line\n",
        "df.index.name = 'line'\n",
        "\n",
        "# save the dataframe to a csv file with separator $\n",
        "df.to_csv('levenshtein_jaccard.csv', sep='$')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# make this a dataframe\n",
        "import pandas as pd\n",
        "\n",
        "dfResults = pd.DataFrame.from_dict(dictResults, orient='index', columns=['Bart', 'DistilBart', 'Bart2', 'Pegasus'])\n",
        "\n",
        "# name the index line\n",
        "dfResults.index.name = 'Line'\n",
        "\n",
        "# save to csv with a $ delimiter including the index\n",
        "dfResults.to_csv('results.csv', sep='$')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
