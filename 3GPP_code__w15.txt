from docx2python import docx2python
import os
import numpy as np
import pandas as pd
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
from sentence_transformers import SentenceTransformer




# suppress warnings
import warnings
warnings.filterwarnings("ignore")




def extractLatencySections(doc):
    strSectionTitle = ""
    dictSections = {}
    listLatency = []
    doc_result = docx2python(doc,paragraph_styles = True, html=True)
    for oneLine in doc_result.text.split('\n'):
        if "<h" in oneLine:
            strSectionTitle = oneLine
            dictSections[strSectionTitle] = []
        if strSectionTitle != "":  
            dictSections[strSectionTitle].append(oneLine)
        keywordsInLine = ["latency", "latencies", "delay"]
        keywordsInSections = ["references", "introduction"]
        if any(word in oneLine.lower() for word in keywordsInLine) and not any(word in strSectionTitle.lower() for word in keywordsInSections): 
            listLatency.append(strSectionTitle)
    for key in list(dictSections.keys()):
        if key not in listLatency:
            del dictSections[key]
    # return the dictionary with the relevant sections
    return dictSections
	
	
	
	
docInputFolder = "./22_standards"
lstAllLines = []
for doc in os.listdir(docInputFolder):    
    if doc.endswith(".docx"):
        print(f"Processing {doc}")
        try: 
            dictSections = extractLatencySections(os.path.join(docInputFolder, doc)) 
            for key in dictSections:
                lstOneLine = [key, doc]
                for line in dictSections[key]:
                    lstOneLine.append(line)
                lstAllLines.append(lstOneLine)
        except Exception as e:
            print(f"Error with {doc}: {e}")
			
			
			
			
model = SentenceTransformer("all-MiniLM-L6-v2")
lstEmbeddings = []
for oneLine in lstAllLines:
    sentences = oneLine[3:]
    embeddings = model.encode(sentences)
    avg_embedding = np.mean(embeddings, axis=0)
    lstOneLine = [oneLine[0], oneLine[1], 2, str(sentences).replace("$", "_").replace("\n", "_"), avg_embedding]
    lstEmbeddings.append(lstOneLine)
	
	
	


df = pd.read_excel("List.xlsx", sheet_name="R_NR")
lstReference = df.values.tolist()
lstReference[0]




lstEmbeddingsRef = []
for oneLine in lstReference:
    sentences = oneLine[0]
    embeddings = model.encode(sentences)
    avg_embedding = embeddings
    lstOneLine = [oneLine[0], 'REF', oneLine[1], oneLine[1], avg_embedding]
    lstEmbeddingsRef.append(lstOneLine)
	



# concatenate the two lists
lstEmbeddingsAll = lstEmbeddings + lstEmbeddingsRef




from sklearn.metrics.pairwise import euclidean_distances
lstDistPos = []
lstDistNeg = []
lstRelevant = []
for oneLine in lstEmbeddings:    
    for oneLineRef in lstEmbeddingsRef:
        if oneLineRef[2] == 1:
            dist = euclidean_distances([oneLine[4]], [oneLineRef[4]])
            lstDistPos.append(dist[0][0])
        if oneLineRef[2] == 0:
            dist = euclidean_distances([oneLine[4]], [oneLineRef[4]])
            lstDistNeg.append(dist[0][0])
    avgDistPos = np.mean(lstDistPos)
    avgDistNeg = np.mean(lstDistNeg)
    if avgDistPos < avgDistNeg:
        oneLine.append(1)
        lstRelevant.append(oneLine)
    else:
        oneLine.append(0)
		
		


import pandas as pd
dfOutput = pd.DataFrame(lstEmbeddings, columns=["Section", "Document", "Class", "Content", "Embedding", "Relevance"])
dfOutput.to_excel("./output__w15__A.xlsx", index=False)




df = pd.read_excel("List.xlsx", sheet_name="LR")
lstRequirements = df.values.tolist()
lstRequirements[0]
lstEmbeddingsReq = []
for oneLine in lstRequirements:
        sentences = oneLine[1]
        embeddings = model.encode(sentences)
        avg_embedding = embeddings
        lstOneLine = [oneLine[0], 'latency', oneLine[1], oneLine[1], avg_embedding]
        lstEmbeddingsReq.append(lstOneLine)
		



lstDist = []
lstRelevantDist = []
for oneLine in lstRelevant:
    for oneLineReq in lstEmbeddingsReq:
        dist = euclidean_distances([oneLine[4]], [oneLineReq[4]])
        lstDist.append([oneLine[0], oneLine[1], oneLineReq[0], dist[0][0], oneLine[3]])
lstDist.sort(key=lambda x: x[2])
for i in range(len(lstDist)):
    print(f"Section {lstDist[i][0]} is close to requirement {lstDist[i][2]} with distance {lstDist[i][3]:.2f}")
    lstRelevantDist.append([lstDist[i][0], lstDist[i][1], lstDist[i][2], lstDist[i][3], lstDist[i][4]])
dfOutput = pd.DataFrame(lstRelevantDist, columns=["Section", "Document", "Requirement", "Distance", "Content"])
dfOutput = dfOutput.sort_values(by=["Section", "Document"])
dfOutput.to_excel("./output_requirements_distances_w15__A.xlsx", index=False)





dfOutput["Distance"] = dfOutput["Distance"].astype(float)
dfGrouped = dfOutput.groupby(["Section", "Document", "Requirement", "Content"])
dfGrouped = dfGrouped.agg({"Distance": "mean"}).reset_index()
dfGrouped.to_excel("./output_requirements_distances_w15__A.xlsx", index=False)