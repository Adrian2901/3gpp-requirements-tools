{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instruct part of the model\n",
    "\n",
    "This part of the code is using the instruct model to write new requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attenton` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e76bb3ead507450e9fe6d97e7e8acbe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/16.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cb41127243c4fed9d5256b189afea6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e4bd4444eb24415a8bf0b1fcf2ce1d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "920cf3ac757f4ef5b31eee88fd152e56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4172add60d54b5aa8230da3a7128fb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45c22dd8dfc44bd0906eec8b3dcfd7b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/172 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a26bf7cc1d1485c8d6f0ec66ecd1463",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/3.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2763924edf6442278f87f7c5f29267c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "363b5bbc4a9c4df48adbc8a4e3096286",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3379a663ab149c6937f1a934989bd88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d66d761f24894cffaf8c5da75f66f299",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/568 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/mnt/d/venvs/standards/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To solve the equation 2x + 3 = 7, you need to isolate the variable x. Here are the steps:\n",
      "\n",
      "1. Subtract 3 from both sides of the equation to get: 2x = 7 - 3, which simplifies to 2x = 4.\n",
      "\n",
      "2. Divide both sides of the equation by 2 to solve for x: x = 4 / 2.\n",
      "\n",
      "So, the solution to the equation 2x + 3 = 7 is x = 2.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-128k-instruct\", \n",
    "    device_map=\"cuda\", \n",
    "    torch_dtype=\"auto\", \n",
    "    trust_remote_code=True, \n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-128k-instruct\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Can you provide ways to eat combinations of bananas and dragonfruits?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Sure! Here are some ways to eat bananas and dragonfruits together: 1. Banana and dragonfruit smoothie: Blend bananas and dragonfruits together with some milk and honey. 2. Banana and dragonfruit salad: Mix sliced bananas and dragonfruits together with some lemon juice and honey.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What about solving an 2x + 3 = 7 equation?\"},\n",
    "]\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "generation_args = {\n",
    "    \"max_new_tokens\": 500,\n",
    "    \"return_full_text\": False,\n",
    "    \"temperature\": 0.0,\n",
    "    \"do_sample\": False,\n",
    "}\n",
    "\n",
    "output = pipe(messages, **generation_args)\n",
    "print(output[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/venvs/standards/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. The payload of the system shall ensure the latency time for robotic aided diagnosis is less than the values specified in TS 22.104, Table A.6.3-1.\n",
      "\n",
      "2. The system's payload shall adhere to the service performance requirements for motion control as outlined in TS 22.104, Table A.6.3-1.\n",
      "\n",
      "3. The payload of the system shall meet the haptic feedback performance requirements as stated in TS 22.104, Table A.6.3-1.\n",
      "\n",
      "4. The system's payload shall guarantee the latency time for robotic aided diagnosis does not exceed the end-to-end values listed in TS 22.104, Table A.6.3-1.\n",
      "\n",
      "5. The payload of the system shall ensure the system's overall performance aligns with the standards set in TS 22.104, Table A.6.3-1.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Generate a requirements starting from 'The payload of the system shall' based on the following text 'The UPF latency time budget related to robotic aided diagnosis, shall be less than the end-to-end values listed in TS 22.104, Table A.6.3-1: Service performance requirements for motion control and haptic feedback.'\"},\n",
    "]\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "generation_args = {\n",
    "    \"max_new_tokens\": 500,\n",
    "    \"return_full_text\": False,\n",
    "    \"temperature\": 0.0,\n",
    "    \"do_sample\": False,\n",
    "}\n",
    "\n",
    "output = pipe(messages, **generation_args)\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The UPF latency time budget related to robotic aided diagnosis, shall be less than the end-to-end values listed in TS 22.104, Table A.6.3-1: Service performance requirements for motion control and haptic feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the UE is in CM-CONNECTED with RRC_INACTIVE state with CN based mobile terminating (MT) communication handling, high latency communication as described in clause 5.31.8 of TS 23.501 [2] is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. The system shall support the UE (User Equipment) in CM-CONNECTED state with RRC_INACTIVE state.\n",
      "2. The system shall handle CN based mobile terminating (MT) communication.\n",
      "3. The system shall be capable of managing high latency communication.\n",
      "4. The system shall apply high latency communication as described in clause 5.31.8 of TS 23.501 [2] when the UE is in CM-CONNECTED state with RRC_INACTIVE state and CN based MT communication handling.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Generate a requirements starting from 'The payload of the system shall' based on the following text 'When the UE is in CM-CONNECTED with RRC_INACTIVE state with CN based mobile terminating (MT) communication handling, high latency communication as described in clause 5.31.8 of TS 23.501 [2] is applied.'\"},\n",
    "]\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "generation_args = {\n",
    "    \"max_new_tokens\": 500,\n",
    "    \"return_full_text\": False,\n",
    "    \"temperature\": 0.0,\n",
    "    \"do_sample\": False,\n",
    "}\n",
    "\n",
    "output = pipe(messages, **generation_args)\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "strContent = \"enerate a requirements starting from: 'The payload of the system shall'\"\n",
    "\n",
    "strC = '''Detected when the UE transitions to CM-CONNECTED state or when the UE will become reachable for paging, e.g. Periodic Registration Update timer. It indicates when the UE becomes reachable for sending downlink data to the UE.\n",
    "The AF may provide the following parameters:\n",
    "1)\tMaximum Latency;\n",
    "2)\tMaximum Response Time;\n",
    "3)\tSuggested number of downlink packets. (see NOTE 5 and NOTE 7).\n",
    "This event requires the Reachability Filter set to UE reachable for DL traffic\" (see clause 5.2.2.3.1-1). For the usage of this event, see clauses 4.2.5.2 and 4.2.5.3.\n",
    "When requesting UE reachability monitoring, the AF may in addition request Idle Status Indication to be included in the UE reachability event reporting.\n",
    "'''\n",
    "\n",
    "strContent = strContent + strC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/venvs/standards/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The system shall initiate a payload transmission when the User Equipment (UE) enters the CM-CONNECTED state or when it becomes accessible for paging, such as during the Periodic Registration Update timer. This event serves as a signal that the UE is now capable of receiving downlink data from the system.\n",
      "\n",
      "The Access Function (AF) is expected to provide certain parameters in this context:\n",
      "\n",
      "1) Maximum Latency: This parameter refers to the longest allowable delay from the time the UE becomes reachable until the system starts sending downlink data.\n",
      "\n",
      "2) Maximum Response Time: This parameter indicates the maximum time the system should take to respond after the UE becomes reachable.\n",
      "\n",
      "3) Suggested number of downlink packets: The AF may suggest a certain number of downlink packets to be sent. This suggestion is based on NOTE 5 and NOTE 7.\n",
      "\n",
      "This event necessitates the Reachability Filter to be set to 'UE is reachable for DL traffic'. This is detailed in clause 5.2.2.3.1-1.\n",
      "\n",
      "For the utilization of this event, refer to clauses 4.2.5.2 and 4.2.5.3.\n",
      "\n",
      "Additionally, when the AF requests UE reachability monitoring, it may also request Idle Status Indication to be part of the UE reachability event reporting.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": strContent},\n",
    "]\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "generation_args = {\n",
    "    \"max_new_tokens\": 500,\n",
    "    \"return_full_text\": False,\n",
    "    \"temperature\": 0.0,\n",
    "    \"do_sample\": False,\n",
    "}\n",
    "\n",
    "output = pipe(messages, **generation_args)\n",
    "print(output[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "standards",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
