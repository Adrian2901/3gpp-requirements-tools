{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca43b662-5b92-47a2-a26b-384ddee87ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wilhelmmeding/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/wilhelmmeding/.local/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The tower is 324 metres tall, about the same height as an 81-storey building. It is the second tallest free-standing structure in France after the Millau Viaduct. It was the first structure to reach a height of 300 metres.\n",
      "\n",
      "Elapsed Time: 1.5 seconds\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "from transformers import pipeline\n",
    "import time\n",
    "\n",
    "# Load the summarization model facebook/bart-large-cnn\n",
    "model_name = 'sshleifer/distilbart-cnn-12-6'\n",
    "\n",
    "# Loading the Model and Tokenizer\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Import the feature extraction pipeline\n",
    "features = pipeline(\"feature-extraction\", model=model_name, tokenizer=model_name, return_tensor=False)\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Tokenize the input text for model processing\n",
    "inputs = tokenizer.encode(\"summarize: \" + text_to_summarize, return_tensors=\"pt\", max_length=512)\n",
    "\n",
    "# Generate a summary using the loaded model\n",
    "outputs = model.generate(inputs, max_length=150, min_length=20, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "\n",
    "# Stop the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the elapsed time\n",
    "elapsed_time = round(end_time - start_time, 1)\n",
    "\n",
    "# Decode the summarized text\n",
    "summary_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(summary_text)\n",
    "\n",
    "# Print the elapsed time\n",
    "print(f\"\\nElapsed Time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea368b2e-2354-4890-b84a-a10c16c7342d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Eiffel Tower is 324 metres tall, about the same height as an 81-storey building, and the tallest structure in Paris. It was the tallest man-made structure in the world until the Chrysler Building in New York City was finished in 1930, then it surpassed it by 5.2 metres.\n",
      "\n",
      "Elapsed Time: 2.6 seconds\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "from transformers import pipeline\n",
    "import time\n",
    "\n",
    "# Load the MEETING-SUMMARY-BART-LARGE model and tokenizer\n",
    "model_name = 'knkarthick/MEETING-SUMMARY-BART-LARGE-XSUM-SAMSUM-DIALOGSUM-AMI'\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Tokenize the input text for model processing\n",
    "inputs = tokenizer.encode(\"summarize: \" + text_to_summarize, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "# Generate a summary using the loaded model\n",
    "outputs = model.generate(inputs, max_length=150, min_length=20, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "\n",
    "# Stop the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the elapsed time\n",
    "elapsed_time = round(end_time - start_time, 1)\n",
    "\n",
    "# Decode the generated summary to obtain the final text\n",
    "summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the generated summary\n",
    "print(summary)\n",
    "\n",
    "# Print the elapsed time\n",
    "print(f\"\\nElapsed Time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "457b52cc-a6d4-4175-bd29-65e48050a22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Eiffel Tower is the Tallest Structure in France, Standing 324 Metres Tallest in the World\n",
      "\n",
      "Elapsed Time: 0.9 seconds\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "from transformers import pipeline\n",
    "import time\n",
    "\n",
    "# Load the summarization model facebook/bart-large-cnn\n",
    "model_name = 'nickmuchi/fb-bart-large-finetuned-trade-the-event-finance-summarizer'\n",
    "\n",
    "# Loading the Model and Tokenizer\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Import the feature extraction pipeline\n",
    "features = pipeline(\"feature-extraction\",model=model_name,tokenizer=model_name,return_tensor=False)\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Tokenize the input text for model processing\n",
    "inputs = tokenizer.encode(\"summarize: \" + text_to_summarize, return_tensors=\"pt\", max_length=512)\n",
    "\n",
    "# Generate a summary using the loaded model\n",
    "outputs = model.generate(inputs, max_length=150, min_length=20, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "\n",
    "# Stop the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the elapsed time\n",
    "elapsed_time = round(end_time - start_time, 1)\n",
    "\n",
    "# Decode the summarized text\n",
    "summary_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(summary_text)\n",
    "\n",
    "# Print the elapsed time\n",
    "print(f\"\\nElapsed Time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0417a1fe-0d8a-426c-a402-c9ff4f92ff32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of PegasusModel were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Elapsed Time: 13.8 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<pad>The Eiffel Tower, built in 1889, is one of the most famous buildings in the world and one of the most famous landmarks in Paris.</s>'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the summarization model google/pegasus-xsum\n",
    "# https://huggingface.co/docs/transformers/main/model_doc/pegasus\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "# import the feature extraction pipeline\n",
    "from transformers import pipeline\n",
    "import time\n",
    "\n",
    "model_name = 'google/pegasus-xsum'\n",
    "\n",
    "#Loading the Model and Tokenizer\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# create the pipeline, which will extract the embedding vectors\n",
    "# the models are already pre-defined, so we do not need to train anything here\n",
    "features = pipeline(\"feature-extraction\",model=\"google/pegasus-xsum\",tokenizer=\"google/pegasus-xsum\",return_tensor = False)\n",
    "\n",
    "inputs = tokenizer.encode(\"summarize: \" + text_to_summarize, return_tensors=\"pt\", max_length=512)\n",
    "\n",
    "outputs = model.generate(inputs, max_length=150, min_length=20, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "\n",
    "# Stop the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the elapsed time\n",
    "elapsed_time = round(end_time - start_time, 1)\n",
    "\n",
    "# Print the elapsed time\n",
    "print(f\"\\nElapsed Time: {elapsed_time} seconds\")\n",
    "\n",
    "# print(outputs) and decode the text\n",
    "tokenizer.decode(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8da9307a-546f-48a7-b6be-5111e0b6f28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requirement text\n",
    "text_to_summarize = 'The tower is 324 metres tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres. Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
